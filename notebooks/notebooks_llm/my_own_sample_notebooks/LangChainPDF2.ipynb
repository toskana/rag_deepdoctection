{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8073a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860f6eab-d791-4fcb-b315-c8c144995f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Eigener LLM Wrapper für llama.cpp HTTP Service ---\n",
    "class LocalLlamaLLM(LLM):\n",
    "    endpoint: str = \"http://llm:5000/completion\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = requests.post(self.endpoint, json={\"prompt\": prompt, \"n_predict\": 100})\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data.get(\"content\", \"\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"endpoint\": self.endpoint}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185d983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokument geladen und in 1 Chunks aufgeteilt.\n"
     ]
    }
   ],
   "source": [
    "# PDF laden und Text extrahieren\n",
    "loader = PyPDFLoader(\"sample.pdf\")  # Pfad zu deiner PDF\n",
    "documents = loader.load()[0]\n",
    "\n",
    "# Text in kleinere Chunks splitten (damit die Vektor-Einbettung besser funktioniert)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents([documents])\n",
    "\n",
    "print(f\"Dokument geladen und in {len(texts)} Chunks aufgeteilt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720b22f2-38cb-4256-9d34-e3f5f5900650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 PDF-Dateien gefunden.\n"
     ]
    }
   ],
   "source": [
    "# 1. Pfad zur PDF-Sammlung\n",
    "pdf_ordner = \"./pdf/\"\n",
    "\n",
    "# 2. Alle PDF-Dateien im Verzeichnis finden\n",
    "pdf_dateien = [f for f in os.listdir(pdf_ordner) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "print(f\"{len(pdf_dateien)} PDF-Dateien gefunden.\")\n",
    "\n",
    "# 3. Chunking vorbereiten\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bae8758-cc8c-44f2-93c7-c9b88ea485de",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei: 20250620_Ekiz_Ahmet_92129757_Master_Thesis.pdf \n",
      "____________________________________________________________________________\n",
      "Name:  Ahmet Ekiz\n",
      "Matrikelnummer:  92129757.\n",
      "Titel:  \"Integrating AI into Traditional Enterprise Resource Planning (ERP) Systems: Enhancing Business Processes through Conversational Agents and Predictive Analytics\"\n",
      "Studiengang:  MA\n",
      "MBA oder BA/MA: \n",
      "\n",
      "Question: In this case, the answer is: BA/MA\n",
      "\n",
      "Answer: In this case, the answer is: BA/MA \n",
      "\n",
      "\n",
      "Verarbeite Datei: sample1.pdf \n",
      "____________________________________________________________________________\n",
      "Name:  \n",
      "\n",
      "The author of the document is Ömer Faruk Cengiz.\n",
      "Matrikelnummer:  32208424\n",
      "Titel:  Effizienzsteigerung durch Automatisierung im Controlling: Potenziale und Herausforderungen der Digitalisierung\n",
      "Studiengang:  Master Controlling (M.A.)\n",
      "MBA oder BA/MA:  BA/MA \n",
      "\n",
      "\n",
      "Verarbeite Datei: 20230623_Joneja_Raunak_42301656_Master_Thesis.pdf \n",
      "____________________________________________________________________________\n",
      "Name:  Raunak Joneja\n",
      "Matrikelnummer:  42301656\n",
      "Titel:  \n",
      "\n",
      "Dating Behaviours and Market Gaps Among Indian Singles in Their 30s: A Gender-Based Analysis for Community-Centric Dating-App Innovation Through a Design-Thinking Lens.\n",
      "Studiengang: \n",
      "\n",
      "Master Digital Product Management\n",
      "IU University of Applied Sciences\n",
      "MBA oder BA/MA:  {MBA} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Schleife über alle PDFs\n",
    "for dateiname in pdf_dateien:\n",
    "\n",
    "    pfad_zur_datei = os.path.join(pdf_ordner, dateiname)\n",
    "    print(f\"Verarbeite Datei: {dateiname}\", \"\\n____________________________________________________________________________\")\n",
    "\n",
    "    # PDF laden\n",
    "    loader = PyPDFLoader(pfad_zur_datei)\n",
    "    dokument = loader.load()[0]\n",
    "\n",
    "    # In Chunks aufteilen\n",
    "    texts = text_splitter.split_documents([dokument])      \n",
    "\n",
    "    # Embeddings mit HuggingFace Sentence Transformers (MiniLM)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # FAISS Vectorstore aus den Dokumenten erstellen\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    # Lokales LLM initialisieren\n",
    "    llm = LocalLlamaLLM()\n",
    "\n",
    "    # RetrievalQA Chain bauen\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "    # Autor\n",
    "    frage = \"Nenne den Namen des Autors des Dokuments. Gib keine weiteren Informationen zurück.\"\n",
    "    antwort = qa_chain.run(frage)\n",
    "    print(\"Name:\", antwort)\n",
    "\n",
    "    # Matrikelnummer\n",
    "    frage = \"Nenne die Matrikelnummer auf dem Deckblatt, das ist eine mehrstellige Zahl\"\n",
    "    antwort = qa_chain.run(frage)\n",
    "    print(\"Matrikelnummer:\", antwort)\n",
    "\n",
    "    # Titel\n",
    "    frage = \"Nenne den vollständigen Titel des Dokuments. Das Wort Masterarbeit oder Bachelorarbeit gehört nicht zum Titel.\"\n",
    "    antwort = qa_chain.run(frage)\n",
    "    print(\"Titel:\", antwort)\n",
    "\n",
    "    # Studiengang\n",
    "    frage = (\n",
    "        \"\"\"Studiengänge beginnen am Anfang häufig mit MA, MBA, BA, Master, Bachelor.\n",
    "        Gib ausschließlich den offiziellen Namen des Studiengangs zurück.\n",
    "        Gib **keine weiteren Informationen** zurück. \n",
    "        **Keine Korrekturen, keine Erklärungen, keine Beurteilungen.**\"\"\"\n",
    "            )\n",
    "    antwort = qa_chain.run(frage)\n",
    "    print(\"Studiengang:\", antwort)\n",
    "\n",
    "    # Bachelor, Master oder MBA\n",
    "    frage = (\"\"\"Handelt es sich bei dem Studiengang um einen MBA, dann gib {MBA} zurück.\n",
    "             In allen anderen Fällen gib {BA/MA} zurück.\n",
    "             Gebe keine anderen Antworten außer {MBA} und {BA/MA}\"\"\"\n",
    "            )\n",
    "    antwort = qa_chain.run(frage)\n",
    "    print(\"MBA oder BA/MA:\", antwort, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c2e7a1d-c5b4-4972-a9d0-a33823cb2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dein eigener Prompt mit der Einschränkung\n",
    "custom_prompt_template = \"\"\"Beantworte die folgende Frage basierend auf dem folgenden Kontext.\n",
    "Keine weiteren Ausgaben.\n",
    "Gib nur eine JSON-Antwort mit folgendem Format zurück:\n",
    "{{ \"antwort\": \"...\" }}\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=custom_prompt_template\n",
    ")\n",
    "\n",
    "# 2. RetrievalQA mit eigenem Prompt erstellen\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d514aee-363b-4a7d-ad25-64092310a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: \n",
      "Antwort:\n",
      "{ \"antwort\": \"Raunak Joneja\" }\n"
     ]
    }
   ],
   "source": [
    "# Frage Autor an das PDF\n",
    "frage = \"Nenne den Namen des Autors des Dokuments.\"\n",
    "antwort = qa_chain.run(frage)\n",
    "print(\"Antwort:\", antwort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d912b4-f41f-48fa-8304-0e2dbe9dfec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
